I"ö<h2 id="about-me">About Me</h2>

<p><img class="profile-picture" src="AIS_square.jpeg" /></p>

<p>Hello! I am a fourth year PhD candidate in the Stanford AI Lab, interested in robot learning, perception, and controls.</p>

<p><strong>Email:</strong> michellelee@cs.stanford.edu</p>

<p><strong>Twitter:</strong> <a href="https://twitter.com/michellearning">michellearning</a></p>

<p>Gates Computer Science Building, Room 132 <br />
353 Serra Mall, Stanford University<br />
Stanford, CA 94305-9025, USA<br /></p>

<h2 id="recent-news">Recent News</h2>

<ul>
  <li>
    <p>One paper submitted to IROS 2021</p>
  </li>
  <li>
    <p>Two paper accepted in ICRA 2021</p>
  </li>
  <li>
    <p>I will be speaking on the panel at the NeurIPS 2020 <a href="https://orlrworkshop.github.io/index.html">Object Representatons for Learning and Reasoning Workshop</a></p>
  </li>
</ul>

<h2 id="invited-talks">Invited Talks</h2>

<ul>
  <li>
    <p>NeurIPS 2020 Object Representations for Learning and Reasoning Workshop, Panel Discussion, December 11, 2020</p>
  </li>
  <li>
    <p>NVIDIA GTC 2020, ‚ÄúDeep Dive with Michelle A. Lee, Making Sense of Vision and Touch: Self- Supervised Learning of Multimodal Representations for Contact-Rich Tasks (ICRA),‚Äù May 14, 2020</p>
  </li>
  <li>
    <p>National Cheng Kung University Institute of Manufacturing Information and Systems Seminar Talk, ‚ÄúMultimodal Fusion for Robust Learning,‚Äù May 7, 2020</p>
  </li>
  <li>
    <p>Stanford Computer Science Faculty Lunch Ph.D. Student Presentation, ‚ÄúMaking Sense of Vision and Touch: Combining Sensor Modalities for Robust Robot Learning,‚Äù March 17, 2020</p>
  </li>
  <li>
    <p>NeurIPS 2019 Workshop on Robot Learning: Control and Interaction in the Real World, ‚ÄúBest Paper Invited Talk: Guided Uncertainty-Aware Policy Optimization,‚Äù December 14, 2019</p>
  </li>
</ul>

<h2 id="publications">Publications</h2>
<ol>
  <li>
    <p><strong>Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks</strong></p>

    <p>Michelle A. Lee*, Yuke Zhu*, Krishnan Srinivasan, Parth Shah, Silvio Savarese, Li Fei-Fei, Animesh Garg, Jeannette Bohg</p>

    <p><em>IEEE International Conference on Robotics and Automation (ICRA), May 2019</em></p>

    <p><span style="color:blue">Best Paper Award in ICRA 2019 </span></p>

    <p><span style="color:blue">Finalist for Best Paper in Cognitive Robotics in ICRA 2019</span>.</p>

    <p><a href="https://arxiv.org/abs/1810.10191">[Paper]</a> <a href="https://sites.google.com/view/visionandtouch">[Website]</a> <a href="https://www.youtube.com/watch?v=usFQ8hNtE8c&amp;feature=emb_title">[Video]</a> <a href="https://github.com/stanford-iprl-lab/multimodal_representation/">[Code and Dataset]</a></p>
  </li>
  <li>
    <p><strong>Variable Impedance Control in End-Effector Space:
An Action Space for Reinforcement Learning in Contact-Rich Tasks</strong></p>

    <p>Roberto Mart√≠n-Mart√≠n, Michelle A. Lee, Rachel Gardner, Silvio Savarese, Jeannette Bohg, Animesh Garg</p>

    <p><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), November 2019</em></p>

    <p><a href="https://arxiv.org/abs/1906.08880">[Paper]</a> <a href="https://stanfordvl.github.io/vices/">[Website]</a> <a href="https://www.youtube.com/watch?v=AozIUIW3Ghs&amp;feature=youtu.be">[Video]</a></p>
  </li>
  <li>
    <p><strong>Making sense of vision and touch: Learning multimodal representations for contact-rich tasks</strong></p>

    <p>Michelle A. Lee, Yuke Zhu, Peter Zachares, Matthew Tan, Krishnan Srinivasan, Silvio Savarese, Li Fei-Fei, Animesh Garg, Jeannette Bohg</p>

    <p><em>IEEE Transactions on Robotics, March 2020</em></p>

    <p><a href="http://ieeexplore.ieee.org/document/9043710">[Paper]</a> <a href="https://github.com/stanford-iprl-lab/multimodal_representation/">[Code and Dataset]</a></p>
  </li>
  <li>
    <p><strong>Guided Uncertainty Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning</strong></p>

    <p>Michelle A. Lee*, Carlos Florensa*, Jonathan Tremblay, Nathan Ratliff, Animesh Garg, Fabio Ramos,  and Dieter Fox</p>

    <p><em>IEEE International Conference on Robotics and Automation (ICRA), May 2020</em></p>

    <p><span style="color:blue">Best Paper Award at the NeurIPS Robot Learning Workshop 2019 </span></p>

    <p><a href="">[Paper]</a> <a href="https://sites.google.com/view/guapo-rl">[Website]</a> <a href="https://www.youtube.com/watch?v=_RGBMdiSMgw">[Video]</a></p>
  </li>
  <li>
    <p><strong>Multimodal Sensor Fusion with Differentiable Filters</strong></p>

    <p>Michelle A. Lee*, Brent Yi*, Roberto Mart√≠n-Mart√≠n, Silvio Savarese, Jeannette Bohg</p>

    <p><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), October 2020</em></p>

    <p><a href="https://arxiv.org/abs/2010.13021">[Paper]</a> <a href="https://sites.google.com/view/multimodalfilter">[Website]</a> <a href="https://github.com/brentyi/multimodalfilter">[Code]</a></p>
  </li>
  <li>
    <p><strong>Detect, Reject, Correct: Crossmodal Compensation of Corrupted Sensors</strong></p>

    <p>Michelle A. Lee, Matthew Tan, Yuke Zhu, Jeannette Bohg</p>

    <p><em>IEEE International Conference on Robotics and Automation (ICRA), June 2021</em></p>

    <p><a href="https://arxiv.org/abs/2012.00201">[Paper]</a> <a href="https://sites.google.com/view/crossmodal-compensation/">[Website]</a></p>
  </li>
  <li>
    <p><strong>Interpreting Contact Interactions to Overcome Failure in Robot Assembly Tasks</strong></p>

    <p>Peter A. Zachares, Michelle A. Lee, Wenzhao Lian, Jeannette Bohg</p>

    <p><em>IEEE International Conference on Robotics and Automation (ICRA), June 2021</em></p>
  </li>
  <li>
    <p><strong>Differentiable Factor Graph Optimization for Learning Smoothers</strong></p>

    <p>Brent Yi, Michelle A. Lee, Alina Kloss, Roberto Mart√≠n-Mart√≠n, Jeannette Bohg</p>

    <p>Submitted to IROS 2021</p>
  </li>
</ol>

:ET