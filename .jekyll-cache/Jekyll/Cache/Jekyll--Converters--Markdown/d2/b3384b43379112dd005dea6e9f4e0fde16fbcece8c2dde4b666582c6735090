I"√<h2 id="about-me">About Me</h2>

<p><img class="profile-picture" src="AIS_square.jpeg" /></p>

<p>Hi! I am a fourth year PhD candidate in the Stanford AI Lab, interested in robot learning, perception, and controls.</p>

<p><strong>Email:</strong> michellelee@cs.stanford.edu</p>

<p><strong>Twitter:</strong> <a href="https://twitter.com/michellearning">michellearning</a></p>

<p>Gates Computer Science Building, Room 132 <br />
353 Serra Mall, Stanford University<br />
Stanford, CA 94305-9025, USA<br /></p>

<h2 id="news">News</h2>
<p>I will be on the panel at the NeurIPS 2020 <a href="https://orlrworkshop.github.io/index.html">Object Representatons for Learning and Reasoning Workshop</a></p>

<p>One paper accepted in IROS 2020</p>

<p>One paper accepted in ICRA 2020</p>

<p>Our journal version of the Multimodal Representation paper has been accepted to T-RO</p>

<h2 id="invited-talks">Invited Talks</h2>

<h2 id="publications">Publications</h2>
<ol>
  <li>
    <p><strong>Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks</strong></p>

    <p>Michelle A. Lee*, Yuke Zhu*, Krishnan Srinivasan, Parth Shah, Silvio Savarese, Li Fei-Fei, Animesh Garg, Jeannette Bohg</p>

    <p><em>IEEE International Conference on Robotics and Automation (ICRA), May 2019</em></p>

    <p><span style="color:blue">Best Paper Award in ICRA 2019 </span></p>

    <p><span style="color:blue">Finalist for Best Paper in Cognitive Robotics in ICRA 2019</span>.</p>

    <p><a href="https://arxiv.org/abs/1810.10191">[Paper]</a> <a href="https://sites.google.com/view/visionandtouch">[Website]</a> <a href="https://www.youtube.com/watch?v=usFQ8hNtE8c&amp;feature=emb_title">[Video]</a> <a href="https://github.com/stanford-iprl-lab/multimodal_representation/">[Code and Dataset]</a></p>
  </li>
  <li>
    <p><strong>Variable Impedance Control in End-Effector Space:
An Action Space for Reinforcement Learning in Contact-Rich Tasks</strong></p>

    <p>Roberto Mart√≠n-Mart√≠n, Michelle A. Lee, Rachel Gardner, Silvio Savarese, Jeannette Bohg, Animesh Garg</p>

    <p><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), November 2019</em></p>

    <p><a href="https://arxiv.org/abs/1906.08880">[Paper]</a> <a href="https://stanfordvl.github.io/vices/">[Website]</a> <a href="https://www.youtube.com/watch?v=AozIUIW3Ghs&amp;feature=youtu.be">[Video]</a></p>
  </li>
  <li>
    <p><strong>Making sense of vision and touch: Learning multimodal representations for contact-rich tasks</strong></p>

    <p>Michelle A. Lee, Yuke Zhu, Peter Zachares, Matthew Tan, Krishnan Srinivasan, Silvio Savarese, Li Fei-Fei, Animesh Garg, Jeannette Bohg</p>

    <p><em>IEEE Transactions on Robotics, March 2020</em></p>

    <p><a href="http://ieeexplore.ieee.org/document/9043710">[Paper]</a> <a href="https://github.com/stanford-iprl-lab/multimodal_representation/">[Code and Dataset]</a></p>
  </li>
  <li>
    <p><strong>Guided Uncertainty Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning</strong></p>

    <p>Michelle A. Lee*, Carlos Florensa*, Jonathan Tremblay, Nathan Ratliff, Animesh Garg, Fabio Ramos,  and Dieter Fox</p>

    <p><em>IEEE International Conference on Robotics and Automation (ICRA), May 2020</em></p>

    <p><a href="">[Paper]</a> <a href="https://sites.google.com/view/guapo-rl">[Website]</a> <a href="https://www.youtube.com/watch?v=_RGBMdiSMgw">[Video]</a></p>
  </li>
  <li>
    <p><strong>Multimodal Sensor Fusion with Differentiable Filters</strong></p>

    <p>Michelle A. Lee*, Brent Yi*, Roberto Mart√≠n-Mart√≠n, Silvio Savarese, Jeannette Bohg</p>

    <p><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), October 2020</em></p>

    <p><a href="https://arxiv.org/abs/2010.13021">[Paper]</a> <a href="https://sites.google.com/view/multimodalfilter">[Website]</a> <a href="https://github.com/brentyi/multimodalfilter">[Code]</a></p>
  </li>
</ol>

:ET